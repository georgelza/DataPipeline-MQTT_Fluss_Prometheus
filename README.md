# Building a Factory machine/sensor monitoring IoT Pipeline. From source through MQTT to Prometheus


## Overview

This originally started as a simple idea, create a IoT JSON packaged payload, publish it to a **MQTT Broker**, consume using a connector into Flink and then down to Fluss and down to the lakehouse configured Paimon storage onto the configured storage technology, which was originally planned as S3.

But we tripped and ended in the proverbial rabbit hole, again, nothing new.


1.  We will publish the payloads generated by the 6 factories grouped into 3 seperate **MQTT Brokers**, based on the regional locatation/distribution, namely `North`, `South` and `East`, consuming the messages using the following source connector.


    1.  Johannesburg        siteId: 101     North
    2.  Cape Town           siteId: 102     South
    3.  Port Elizabeth      siteId: 103     East
    4.  Pretoria            siteId: 104     North
    5.  Cape Town           siteId: 102     South
    6.  George              siteId: 105     East

For more sites see end of the file.   

2.  Once on the MQTT Topics we will consume the payloads using a MQTT/Flink connector into `hive_catalog.mqtt.factory_iot_#` tables. This is done using `<root>/devlab0/creFlinkFlows/2.1.creMQTTSource.sql`
   
from our **Apache Flink** cluster and ingest the data into our `hive_catalog.mqtt.factory_iot_#`  (each table mimicing a siteId) tables. We will then insert the data into our `fluss_catalog.iot.*` selecting from the `hive_catalog.mqtt.factory_iot_*` tables.

NOTE: This [MQTT-Flink-Source-connector-v3](https://github.com/georgelza/MQTT-Flink-Source-connector-v3) is an example connector... Do not assume that it is large enterprise producion ready.

1.  After this we will consolidate all into one fluss based table namely, `factory_iot_unnested` table, during this insert we also unnest/flatten the data structure.
   
2.  We will now create 3 flink/output tables using the "beta" [prometheus](https://prmetheus.io) [Apache Flink sql connector](https://github.com/apache/flink-connector-prometheus/pull/22). (see Build various containers, step 3 re the beta prometheus sink connector). 
   
3.  We will then run 3 jobs to insert the `siteId=101`, `siteId=102` and `siteId=103` into the above created tabless. During this step we assign the metric label, you can run all 3 flink inserts as one label or you can call it say **north_metrics**, **south_metrics** and **east_metrics**.
   
4.  At this point we have data flowing from source into mqtt, via flink into fluss, then sinked down into prometheus using a flink connector. At this stage we now have the entire pipeline defined, allowing us to created fancy Grafana dashboards.


[GIT Repo:](https://github.com/georgelza/DataPipeline-MQTT_Fluss_Prometheus)


### NOTE:  Pretty important, Prometheus and time

The Prometheus container runs as UTC time (I've tried to use environment setting to change it, no success). But the following always applies, so make sure to consider that in anything re prometheus destined data.

1. Firstly [Prometheus](https://prometheus.io) does not allow old data to be published onto the remote_writer that is to far in the past, similarly it also does not allow for time travellers from the future... Creating data with a time stamp to far into the future.

2. My data generator can be used in various configurations... i.e.: to just push metrics into MQTT and down into flink and onwards to Fluss. Where it can then be used to do analytics etc. on. In this scenario it can be configured to first create historic data and then go into an current mode.

3. However, if we are going to sink the data into Prometheus (as per this blog), be aware, as per above. Prometheus does not allow for data to be created in the past. To cater for this scenario... you need to set the `RUNHISTORIC=0` in the `<root>/app_iot#/site$.sh` file to disable the historic data generator functionality.

4. With Regard to back filling data for Prometheus, see: [backfilling-from-openmetrics-format](https://prometheus.io/docs/prometheus/latest/storage/#backfilling-from-openmetrics-format). For more see `<root>/otl/`


### Prometheus Sink connector

Ok, so before we go much further. This Prometheus sink connector uses the Prometheus servers remote writer framework. 

Baiscally this would normally be one prometheus host configured as a reciever and another configured to remote write (push) metrics to the receiver. 

For the Prometheus sink connector to work we configure our prometheus server to be a receiver. To eanble this on the receiver side the following environment variable `"--web.enable-remote-write-receiver"` is passed in. this then exposes a receiver process at `http://prometheus:9090/api/v1/write`

We then configure our Prometheus sink connector by setting `'metric.endpoint-url' = 'http://prometheus:9090/api/v1/write'`.

`http://prometheus:9090` being our receiver Prometheus server and `/api/v1/write` being the default url for the `remote-write-receiver`.


### Test Prometheus Stack

`<root>/devlab0/creFlinkFlows/2.0.crePromSource.sql` and `3.0.creProm*` are simple example of a table create and insert commands to see if all is working.


### Flink Catalog

We're still using our **Apache Hive Metastore** as catalog with a **PostgreSQL** database for backend storage.
See below for version information.


## Modules and Versions

- Ubuntu 24.04 LTS

- MQTT Broker - eclipse-mosquitto:2.0.21

- Apache Flink 1.20.1 - Java 17

- Apache Fluss 0.6.0  (With Zookeeper 3.9.2)

- Hadoop File System (HDFS) 3.3.5 build (OpenJDK11) on Ubuntu 20.04 LTS

- Hive Metastore 3.1.3 on Hadoop 3.3.5 (OpenJDK8) on Ubuntu 24.04 LTS

- PostgreSQL 12

- Python 3.13

- Prometheus v3.3.0
  
- Grafana 11.6.1


## Our various IoT Payloads formats.

### Basic min IoT Payload, produced by Factories 101 and 104

```json5
{
    "ts": 1729312551000, 
    "metadata": {
        "siteId": 101, 
        "deviceId": 1004, 
        "sensorId": 10034, 
        "unit": "BAR"
    }, 
    "measurement": 120
}
```

### Basic min IoT Payload, with a human readable time stamp & location added, produced by Factories 102 and 105

```json5
{
    "ts": 1713807946000, 
    "metadata": {
        "siteId": 102, 
        "deviceId": 1008, 
        "sensorId": 10073, 
        "unit": "Liter", 
        "ts_human": "2024-04-22T19:45:46.000000", 
        "location": {
            "latitude": -33.924869, 
            "longitude": 18.424055
        }
    }, 
    "measurement": 25
}
```

### Complete IoT Payload, with deviceType tag added, produced by Factories 103 and 106

```json5
{
    "ts": 1707882120000, 
    "metadata": {
        "siteId": 103, 
        "deviceId": 1014, 
        "sensorId": 10124, 
        "unit": "Amp", 
        "ts_human": "2024-02-14T05:42:00.000000", 
        "location": {
            "latitude": -33.9137, 
            "longitude": 25.5827
        }, 
        "deviceType": "Hoist_Motor"
    },
    "measurement": 24
}
```


## To run the project.

### See various configuration settings and passwords in:

0. devlab0/docker_compose.yml

1. .pwd in app_iot# in siteX.sh

2. devlab0/.env

3. devlab0/conf/hive.env

4. devlab0/conf/hive-site.xml


### Download containers and libraries

1. cd infrastructure

2. make pullall

3. make buildall


### Build various containers

1. cd devlab0

2. ./getlibs.sh

3. One lib/.jar file not downloadable at this stage, using the #2 command is our primary prometheus sink jar, this is because as we stand right now this is still in development and a PR, see: [flink-connector-prometheus](https://github.com/apache/flink-connector-prometheus/pull/22). As such I'm including it in the `<root>/devlab0/prometheus_jar` diretory, please copy/move this into the `<root>/devlab0/conf/flink/lib/flink` directory. Also rememver to copy the mqtt-*jar source connector jars to this directory.

4. If you want to run the load generators as docker apps ->  make buildapp
   Otherwise skip step 4 and continue to 5.

5. Now, to run it please read README.md in `<root>/devlab0/README.md` file.


### Mosquito Access control

[Authentication methods](https://mosquitto.org/documentation/authentication-methods/)

I decided to behave an enable authentification on the MQTT Broker, to accomplish this we need a password file with a username and password contained. To prepare for this follow the following steps.

Start by executing `<root>/devlab#/make run`

This will start the stack, now execute the below.

Now execute
    `docker compose exec mqtt_broker_north /bin/sh`

Followby by executing inside the container.

    `mosquitto_passwd -c /mosquitto/config/password_file mqtt_dev`

    I used `abfr24` as password, this will match the `.pws` in `<root>/app_mqttiot1/.pws`

First make sure the `<root>/devlab#/conf/mqtt/[<east/north/south>]/mosquitto.conf` contain the below


**North**
```shell
    persistence true
    listener 1883
    persistence_location /mosquitto/data
    log_dest file /mosquitto/log/mosquitto.log
    password_file /mosquitto/config/password_file
```

**South**
```shell
    persistence true
    listener 1884
    persistence_location /mosquitto/data
    log_dest file /mosquitto/log/mosquitto.log
    password_file /mosquitto/config/password_file
```

**East**
```shell
    persistence true
    listener 1885
    persistence_location /mosquitto/data
    log_dest file /mosquitto/log/mosquitto.log
    password_file /mosquitto/config/password_file
```


Now execute `make down`, once the stack is stopped execute the below.

    Copy the files located into the `<root>/devlab0/conf/mqtt/north/` to `../south/` & `../east/` directories.


I use both MQTT Explorer and MQTT.fx to see whats happening on MQTT Brokers.


4. Now, to run it please read README.md in `<root>/devlab0/README.md` file.


## Projects / Components

- [MQTT](https://mqtt.org)
  
- [Apache Flink](https://flink.apache.org)

- [Ververica](https://www.ververica.com)

- [What is Fluss](https://alibaba.github.io/fluss-docs/)

- [Fluss Overview](https://alibaba.github.io/fluss-docs/docs/install-deploy/overview/)

- [What is Fluss docs](https://alibaba.github.io/fluss-docs/docs/intro/)

- [Fluss Project Git Repo](https://github.com/alibaba/fluss)

- [Introduction to Fluss](https://www.ververica.com/blog/introducing-fluss)

- [Apache Paimon](https://paimon.apache.org)

- [Apache Parquet File format](https://parquet.apache.org)


### Flink Libraries

As I am always travelling while writing these blog's and did not want to pull the libraries on every build I decided to downlaod them once into the below directory and then mount them into container. Just a different way less bandwidth and also slightly faster builds.

The `devlab0/conf/flink/lib/*` directories will house our Java libraries required by our Flink stack. 

Normally I'd include these in the Dockerfile as part of the image build, but during development it's easier if we place them here and then mount the directories into the containers at run time via our `docker-compose.yml` file inside the volume specification for the flink-* services.

This makes it simpler to add/remove libraries as we simply have to restart the flink container and not rebuild it.

Additionally, as the `jobmanager`, `taskmanager` use the same libraries doing it tis way allows us to use this one set, thus also reducing the disk space and the container image size.

The various files are downloaded by executing the `getlibs.sh` file located in the `devlab0/` directory.


### Flink base container images for 1.20.1 (manual pull from `hub.docker.com`)

- docker pull arm64v8/flink:1.20.1-scala_2.12-java11


### Self Build Flink container

- [Master Flink download](https://flink.apache.org/downloads/#apache-flink-1201)

- [Flink 1.20.1 binaries](https://www.apache.org/dyn/closer.lua/flink/flink-1.20.1/flink-1.20.1-bin-scala_2.12.tgz)


## Uncategorized notes and Articles

- [Apache Flink FLUSS](https://www.linkedin.com/posts/polyzos_fluss-is-now-open-source-activity-7268144336930832384-ds87?utm_source=share&utm_medium=member_desktop)


- [Apache Flink Deployment](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/)    
    

- [Troubleshooting Apache Flink SQL S3 problems](https://www.decodable.co/blog/troubleshooting-flink-sql-s3-problems)

### HDFS Cluster

- [Setting Up an HDFS Cluster with Docker Compose: A Step-by-Step Guide](https://bytemedirk.medium.com/setting-up-an-hdfs-cluster-with-docker-compose-a-step-by-step-guide-4541cd15b168)
- [Deploying Hadoop using Docker](https://medium.com/@garin.sanny07/hadoop-cluster-55477505d0ff)

- [Installing the Hadoop Stack using Docker](https://hackmd.io/@silicoflare/docker-hadoop)


### Flink Cluster

- [how-to-set-up-a-local-flink-cluster-using-docker](https://medium.com/marionete/how-to-set-up-a-local-flink-cluster-using-docker-0a0a741504f6)


### RocksDB

- [Using RocksDB State Backend in Apache Flink: When and How](https://flink.apache.org/2021/01/18/using-rocksdb-state-backend-in-apache-flink-when-and-how/)


### Log4J Logging levels

- [Log4J Logging Levels](https://logging.apache.org/log4j/2.x/manual/customloglevels.html)
    

- The Flink jobmanager and taskmanager log levels can be modified by editing the various `devlab#/conf/*.properties` files. Remember to restart your Flink containers.


### Great quick reference for docker compose

- [A Deep dive into Docker Compose by Alex Merced](https://dev.to/alexmercedcoder/a-deep-dive-into-docker-compose-27h5)


### Consider using secrets for sensitive information

- [How to use sectrets with Docker Compose](https://docs.docker.com/compose/how-tos/use-secrets/)


### Enabling Prometheus monitoring on Minio with grafana dashboard

- [Enabling Prometheus Scraping of Minio](https://min.io/docs/minio/linux/operations/monitoring/metrics-and-alerts.html)


- [Grafana Dashboards](https://min.io/docs/minio/linux/operations/monitoring/grafana.html#minio-server-grafana-metrics)


## Misc Notes

The various `REAMDE.md` utilises markdown syntax. You can refer to `https://markdownlivepreview.com` & `https://dillinger.io` for more information, examples.

To view a mardown file, `https://jumpshare.com/viewer/md` or if you using Visual Studio Code search for markdown as a module and try some of them.


### Additional Sites

I've added the below sites with devices and sensors into `<root>/app_iot1/conf/addFull.json`

    7.   East London         siteId: 107         East
    8.   Richards Bay        siteId: 108         East
    9.   Mosselbay           siteId: 109         East
    10.  Saldanha            siteId: 110         West
    11.  Port Nolloth        siteId: 111         West
    12.  Kimberley           siteId: 112         Central
    13.  Bloemfontein        siteId: 113         Central
    14.  Welkom              siteId: 114         Central

Note: The presence of the 2 new regions, West and Central. If you decide to add all of these also add the required topics and tables.
They were created as various copies of 101->106, with maybe some slight changes here and there.



### By:

George

[georgelza@gmail.com](georgelza@gmail.com)

[George on Linkedin](https://www.linkedin.com/in/george-leonard-945b502/)

[George on Medium](https://medium.com/@georgelza)

